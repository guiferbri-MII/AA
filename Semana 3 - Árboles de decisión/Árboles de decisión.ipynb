{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesión de trabajo con Jupyter aplicaremos los conceptos presentados en el módulo sobre árboles de decisión al conjunto de datos Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesado de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicios vamos a utilizar de nuevo el conjunto de datos [datos Iris](https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos) que contiene información de tres especies de la flor Iris: Iris setosa, Iris virginica e Iris versicolor. Recordemos que en este conjunto de datos hay información de 150 muestras de flores, 50 de cada tipo, sobre el largo y el ancho del pétalo y el sépalo, medidos en centímetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos Iris incluido en Scikit-learn, usando la función [`load_iris()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) de la biblioteca `sklearn.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenamos los campos que resultan de interés para este ejercicio en variables distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, X_names, y_names = \\\n",
    "    iris.data, iris.target, iris.feature_names, iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a recordar cómo se distribuían estos datos y para ello los visualizaremos usando la librería `matplotlib`. Para esto, consideramos un par de características y presentamos en un gráfico 2D un conjunto de puntos que representan cada uno de los ejemplos del conjunto de datos. Para diferenciar los valores de clasificación usamos distintas formas y colores para cada uno de ellos: cuadrados rojos para el primer valor de clasificación (Iris setosa), círculos verdes para el segundo valor de clasificación (Iris versicolor) y rombos azules para el tercer valor de clasificación (Iris virgínica)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la biblioteca [`pyplot`](https://matplotlib.org/api/pyplot_api.html) de la librería `matplotlib` con el nombre `plt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso para dibujar los datos consiste en limpiar el lienzo (`plt.clf()`) y para cada terna formada por el valor de clasificación (`range(len(clases))`), la forma (`\"soD\"`) y el color (`\"rgb\"`) deseados, recoger los valores de las características a representar (`c1` y `c2`) de todos los datos con el valor de clasificación considerado. Estos puntos se representan con el método (`plt.scatter`) con la forma y el color fijados. Finalmente etiquetamos los ejes horizontal y vertical con los nombres de las características representadas.\n",
    "\n",
    "Este proceso se puede definir como el efecto de una función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion_grafica(datos,caracteristicas,objetivo,clases,c1,c2):\n",
    "    for tipo,marca,color in zip(range(len(clases)),\"soD\",\"rgb\"):\n",
    "        plt.scatter(datos[objetivo == tipo,c1],\n",
    "                    datos[objetivo == tipo,c2],marker=marca,c=color)\n",
    "    plt.xlabel(caracteristicas[c1])\n",
    "    plt.ylabel(caracteristicas[c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos esta función para representar gráficamente la distribución de los datos con respecto a cada pareja de características. Eso se consigue dividiendo el lienzo en seis trozos, en 2 filas y 3 columnas con las correspondientes llamadas al método `plt.subplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion_conjunta():\n",
    "    plt.clf()\n",
    "    plt.rcParams[\"figure.figsize\"] = [18,12]\n",
    "    plt.subplot(231)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,0,1)\n",
    "    plt.subplot(232)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,0,2)\n",
    "    plt.subplot(233)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,0,3)\n",
    "    plt.subplot(234)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,1,2)\n",
    "    plt.subplot(235)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,1,3)\n",
    "    plt.subplot(236)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,2,3)\n",
    "    plt.subplots_adjust(wspace=0.2,hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "representacion_conjunta()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos gráficos nos sirven para ver que las características que mejor separan los datos (y por tanto las que mejor sirven para clasificarlos) son la longitud y anchura del pétalo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo de aprendizaje de árboles de decisión que está incluido en *scikit_learn* es CART. Al igual que otros modelos de clasificación, está implementado como una clase, [`DecisionTreeClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), de la librería [`tree`](http://scikit-learn.org/stable/modules/tree.html). Importamos esta clase y creamos una instancia del algoritmo de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree1_clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos de clasificación incluidos en *scikit_learn* sirven para conjuntos de datos numéricos, tal y como el ejemplo que estamos considerando. En caso de que nuestro conjunto de datos tuviese características categóricas, tendríamos que preprocesarlos para transformar sus valores literales en valores númericos.\n",
    "\n",
    "Como otros modelos de clasificación en *scikit_learn*, la clase `DecisionTreeClassifier` dispone de un método para entrenar el modelo (`fit`) al que hay que pasar el conjunto de entrenamiento con dos argumentos, un array con el conjunto de datos y otro con el valor de clasificación para esos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1_clf.fit(X_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al enternar el modelo, obtenemos como resultado la descripción del árbol de decisión con los valores de los parámetros que se han usado en el algoritmo de aprendizaje. Estos valores servirán para ajustar el proceso de aprendizaje y los veremos más adelante. Veamos ahora el aspecto que tiene el árbol de decisión aprendido. \n",
    "\n",
    "Para esto vamos a usar el método [`export_graphviz`](http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html) de la librería `tree`. Este método genera una descripción del árbol de decisión en el formato `dot` del software de visualización de grafos [`graphviz`](https://www.graphviz.org/). En la llamada de este método pasamos como argumento el modelo de clasificación entrenado, el fichero en el que queremos almacenar el resultado (con la extensión `dot`) y otras opciones como el array de nombres de las características y detalles relacionados con la representación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "    tree1_clf,\n",
    "    out_file=\"iris_tree1.dot\",\n",
    "    feature_names=iris.feature_names,\n",
    "    rounded=True,\n",
    "    filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un grafo en formato `dot` puede ser dibujado en distintos formatos con la utilidad del mismo nombre disponible en el sofware de visualización de grafos [`graphviz`](https://www.graphviz.org/download/). Para usarlo ejecutamos la utilidad `dot` mediante una llamada al sistema con los parámetros adecuados para generar el gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng iris_tree1.dot -o iris_tree1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver el árbol de decisión generado, lo incluimos como una imagen externa en la paleta de dibujo de `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "image = plt.imread(\"iris_tree1.png\")\n",
    "height, width, depth = image.shape\n",
    "dpi = 70\n",
    "figsize = width / float(dpi), height / float(dpi)\n",
    "plt.figure(figsize=figsize).add_axes([0, 0, 1, 1])\n",
    "plt.axis('off')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir la función `show_tree` para realizar esta tarea de forma más comoda las próximas veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tree(tree_clf,features):\n",
    "    export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"tree_clf.dot\",\n",
    "        feature_names=features,\n",
    "        rounded=True,\n",
    "        filled=True)\n",
    "    !dot -Tpng tree_clf.dot -o tree_clf.png\n",
    "    plt.clf()\n",
    "    dpi = 70\n",
    "    image = plt.imread(\"tree_clf.png\")\n",
    "    height, width, depth = image.shape\n",
    "    figsize = width / float(dpi), height / float(dpi)\n",
    "    plt.figure(figsize=figsize).add_axes([0, 0, 1, 1])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    \n",
    "show_tree(tree1_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada nodo interno del árbol anterior muestra información sobre el criterio de separación elegido, el valor del grado de dispersión o impureza para dicho nodo, el número de ejemplos del conjunto de entrenamiento asociados a ese nodo y la distribución de los valores de clasificación para dichos ejemplos. \n",
    "\n",
    "Por ejemplo, el nodo inicial nos indica que el primer criterio de separación elegido consiste en comprobar si la anchura del pétalo es menor o igual que `0.8` centímetros. Eso quiere decir que éste es el criterio de separación que mejor clasifica los datos iniciales. El conjunto de datos inicial, que está asociado a este nodo, tiene un grado de dispersión (en este caso el índice de Gini) de `0.667`, está formado por `150` muestras (todas las del conjunto de entrenamiento) y su distribución en los tres valores de clasificación es `[50,50,50]`. El valor del índice de Gini para este nodo se puede confirmar con esta información:\n",
    "\n",
    "$$G({\\cal S}) = 1 - \\sum_{j=1}^k {p_j}^2 = 1 - \\sum_{j=1}^3 (\\frac{1}{3})^2 = 1 - \\frac{3}{9} = \\frac{2}{3} \\approx 0.667$$\n",
    "\n",
    "Como se puede observar del resultado del primer criterio de separación, hay un total de `50` muestras cuyo valor de anchura de pétalo es menor o igual que `0.8` y otras `100` que están por encima. El conjunto de muestras asociado a la rama de la izquierda tiene un índice de Gini de `0` pues todas son del tipo Iris setosa. Por tanto, el nodo de la izquierda es un nodo hoja que tiene asociado el valor de clasificación \"Iris setosa\". Su color está asociado con ese valor de clasificación.\n",
    "\n",
    "Por otro lado, hay un total de `100` muestras cuyo valor de anchura de pétalo es mayor que `0.8`. Estas son las muestras asociadas al siguiente nodo de la rama de la derecha. La distribución de los valores de clasificación en este conjunto es `[0,50,50]`, de aquí que su grado de impureza sea `0.5`. De nuevo, este dato se puede confirmar con los datos disponibles usando la fórmula del índice de Gini.\n",
    "\n",
    "Dado que en este nodo las muestras no están correctamente clasificadas, hay que buscar otro criterio de separación. En este caso consiste en comprobar si la anchura del pétalo es menor o igual que `1.75` centímetros. En este caso el conjunto de datos (las `100` muestras de Iris virgínica e Iris versicolor), queda dividido en dos subconjuntos. El de la izquierda está formado por `54` muestras (`49` de la clase Iris versicolor y `5` de la clase Iris virgínica), con un índice de impureza de `0.168`. El de la derecha está formado por `46` muestras (`1` de la clase Iris versicolor y `45` de la clase Iris virgínica), con un índice de impureza de `0.043`. Aunque en estos nodos no se tiene una clasificación definitiva de los datos, ésta está casi determinada, lo que se refleja en el color asociado a estos nodos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos visto el grado de dispersión se ha calculado usando el índice de Gini. Este es el valor por defecto del algoritmo de aprendizaje de árboles de decisión en *scikit_learn*. Podemos cambiar este criterio usando el parámetro `criterion` del constructor de la clase `DecisionTreeClassifier`. Los valores disponibles son `gini` para usar el índice de Gini y `entropy` para usar la ganancia de información. Veamos como queda el árbol de decisión construido usando la ganancia de información:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_clf = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "tree2_clf.fit(X_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si visualizamos este árbol de decisión, observaremos que es prácticamente igual al anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree2_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de los criterios de separación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora con más detalle cómo se buscan los mejores criterios de separación en el algoritmo CART. Estos criterios de separación consisten en comparar el valor de una característica con una cota, separando las muestras que tienen un valor menor o igual que la cota de aquellas que tienen un valor mayor que la cota. El funcionamiento del criterio es simple, al igual que la forma de calcular el mejor de todos: el que tiene un valor más bajo de la suma ponderada de los grados de impureza. Veamos a continuación como se escogen las cotas con respecto a las cuales se definen estos criterios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijémonos en una característica en concreto, por ejemplo la longitud del pétalo. Primero debemos considerar todas las muestras asociadas al nodo en el que estamos buscando el criterio de separación, fijándonos en el valor de esta característica junto con su valor de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_pair = [(X_data[i,2],y_data[i]) for i in range(y_data.shape[0])]\n",
    "\n",
    "Xy_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación ordenamos estas muestras con respecto al valor de la característica. Para ello es suficiente con ordenar la lista de parejas anterior con la función `sorted`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xy_sorted = sorted(Xy_pair)\n",
    "\n",
    "Xy_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora buscamos los valores de la característica entre los que el valor de clasificación cambia. Una forma de hacer esto consiste en emparejar cada dato con el siguiente y considerar aquellas parejas en las que el valor de clasificación cambia. Para cada pareja encontrada nos interesa el valor medio entre los valores de la característica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cotas = []\n",
    "for ((vi,ci),(vj,cj)) in zip(Xy_sorted,(Xy_sorted[1:])):\n",
    "    if ci != cj:\n",
    "        cotas.extend([(vj+vi)/2])\n",
    "        \n",
    "cotas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se tienen los posibles valores de las cotas, basta con averiguar cual es la que disminuye en mayor medida el grado de impureza. Por ejemplo, para la primera cota candidata, se calcula la distribución de las muestras que tienen un valor de  la característica menor o igual que la cota (`dist_left`) y la distribución de las muestras que tienen un valor de la característica mayor que la cota (`dist_right`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_left = [len([1 for j in range(150) \n",
    "                    if X_data[j][2] <= 2.45\n",
    "                    and y_data[j] == i]) for i in range(3)] \n",
    "\n",
    "dist_right = [len([1 for j in range(150) \n",
    "                     if X_data[j][2] > 2.45\n",
    "                     and y_data[j] == i]) for i in range(3)]\n",
    "\n",
    "print(\"Distribución rama izquierda: \",dist_left)\n",
    "\n",
    "print(\"Distribución rama derecha: \",dist_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hay que calcular la mejora en el grado de dispersión para cada criterio de separación. El índice de Gini de una distribución de datos se puede calcular con la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(xs):\n",
    "    n = sum(xs)\n",
    "    ps = [x/n for x in xs]\n",
    "    return 1-sum(p**2 for p in ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El grado de dispersión ponderado después de usar el criterio de separación es:\n",
    "\n",
    "$$\\sum^r_{i=1} \\frac{|{\\cal S}_i|}{|{\\cal S}|} G({\\cal S}_i)$$\n",
    "\n",
    "que podemos calcular fácilmente en el caso que nos ocupa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(dist_left)*gini(dist_left)+sum(dist_right)*gini(dist_right))/150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función calcula el grado de dispersión (índice de Gini) de un criterio de separación a partir de una muestra de datos, sus valores de clasificación, la característica considerada en el criterio y la cota con respecto a la que se compara el valor de esta característica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indice_gini(X_data,y_data,caracteristica,cota):\n",
    "    n = X_data.shape[0]\n",
    "    dist_left = [len([1 for j in range(n) \n",
    "                        if X_data[j][caracteristica] <= cota\n",
    "                        and y_data[j] == i]) for i in range(3)]    \n",
    "    dist_right = [len([1 for j in range(n) \n",
    "                         if X_data[j][caracteristica] > cota\n",
    "                         and y_data[j] == i]) for i in range(3)]\n",
    "    return (sum(dist_left)*gini(dist_left)+sum(dist_right)*gini(dist_right))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos esta función para todas las cotas en las que se produce un cambio en el valor de clasificación de los datos, podemos observar que la mejor de todas es la primera (`2.45`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[indice_gini(X_data,y_data,2,c) for c in cotas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, para elegir el criterio de separación hay que considerar todas las características del conjunto de datos, determinar los posibles valores de las cotas para cada una de ellas, y calcular el grado de dispersión en todos estos casos. El criterio de separación que tenga un valor mínimo del grado de dispersión será el elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selecciona_criterio(X_data,X_names,y_data):\n",
    "    # Número de características en cada dato\n",
    "    num_caracteristicas = X_data.shape[1]\n",
    "    # Número de instancias en el conjunto de datos\n",
    "    num_ejemplos = X_data.shape[0]\n",
    "    # Mejor característica obtenida hasta el momento\n",
    "    mejor_caracteristica = 0\n",
    "    # Mejor cota para la mejor característica obtenida hasta el momento\n",
    "    mejor_cota = 0\n",
    "    # Grado de dispersión para la mejor cota de la mejor característica\n",
    "    mejor_grado_dispersion = 1\n",
    "    for caracteristica in range(num_caracteristicas):\n",
    "        Xy_pair = [(X_data[i,caracteristica],y_data[i]) for i in range(num_ejemplos)]\n",
    "        Xy_sorted = sorted(Xy_pair)\n",
    "        cotas = []\n",
    "        for ((vi,ci),(vj,cj)) in zip(Xy_sorted,(Xy_sorted[1:])):\n",
    "            if ci != cj:\n",
    "                cotas.extend([(vj+vi)/2])\n",
    "        for cota in cotas:\n",
    "            grado_dispersion = indice_gini(X_data,y_data,caracteristica,cota)\n",
    "            if grado_dispersion <= mejor_grado_dispersion:\n",
    "                mejor_grado_dispersion = grado_dispersion\n",
    "                mejor_cota = cota\n",
    "                mejor_caracteristica = caracteristica\n",
    "    return (mejor_caracteristica,X_names[mejor_caracteristica],mejor_cota)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos esta función con el conjunto de datos inicial, veremos cual es el mejor criterio de separación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selecciona_criterio(X_data,X_names,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro método para escoger el mejor criterio de separación no siempre obtiene el mismo resultado que el que viene implementado en *scikit_learn*. Esta circunstancia es así puesto que el algoritmo implementado en *scikit_learn* trata de otra forma las situaciones en las que hay varias muestras con el mismo valor de una característica y distintos valores de clasificación, tomando decisiones aleatorias en algunos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobreajuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora como tratar el sobreajuste en los árboles de decisión. Si analizamos los árboles obtenidos anteriormente veremos que a partir de la segunda capa los criterios de separación buscan distinguir una cantidad muy pequeña de muestras. De hecho, en la rama de la derecha hay un desarrollo considerable (dos nodos internos) para distinguir una única muestra entre las demás. Esta es una característica habitual en los árboles de decisión sobreajustados. El problema del sobreajuste se corresponde con un bajo rendimiento del modelo aprendido en conjuntos de datos independientes del conjunto de entrenamiento.\n",
    "\n",
    "Para apreciar cómo disminuye el sobreajuste del árbol de decisión aprendido será necesario considerar previamente dos conjuntos independientes de muestras: un conjunto de entrenamiento para construir el modelo y un conjunto de prueba para evaluarlo.\n",
    "\n",
    "Para hacer esto usamos la función `train_test_split` de la librería `model_selection` incluida en *scikit_learn*. En este caso consideramos un conjunto de prueba con el 25% de los datos originales y un conjunto de entrenamiento con el 75% restante. Para apreciar las mejoras obtenidas con las distintas estrategias de reducción de sobreajuste, fijamos la semilla del generador de números aleatorios usado para realizar este reparto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "  train_test_split(X_data,y_data,test_size = 0.25,\n",
    "                   random_state=462)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente gráfico vemos de forma conjunta las representaciones de las dos primeras características del conjunto de datos inicial, el conjunto de entrenamiento y el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [18,6]\n",
    "plt.clf()\n",
    "splt = plt.subplot(131)\n",
    "splt.set_xlim(4,8)\n",
    "splt.set_ylim(1.8,4.5)\n",
    "splt.set_title(\"Conjunto de datos\")\n",
    "representacion_grafica(X_data,X_names,y_data,y_names,0,1)\n",
    "splt = plt.subplot(132)\n",
    "splt.set_xlim(4,8)\n",
    "splt.set_ylim(1.8,4.5)\n",
    "splt.set_title(\"Conjunto de entrenamiento\")\n",
    "representacion_grafica(X_train,X_names,y_train,y_names,0,1)\n",
    "splt = plt.subplot(133)\n",
    "splt.set_xlim(4,8)\n",
    "splt.set_ylim(1.8,4.5)\n",
    "splt.set_title(\"Conjunto de prueba\")\n",
    "representacion_grafica(X_test,X_names,y_test,y_names,0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora construimos un árbol de decisión con el conjunto de entrenamiento tal y como hemos hecho anteriormente. En este caso también fijamos el valor de la semilla del generador de números aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3_clf = DecisionTreeClassifier(random_state=10)\n",
    "\n",
    "tree3_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación gráfica del árbol obtenido es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree3_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que ocurría con el conjunto de datos original, el árbol obtenido tiene ramificaciones para distinguir conjuntos muy pequeños de datos. Veamos su rendimiento en el conjunto de prueba, para esto usamos el método `predict` del modelo obtenido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos estos resultados con los valores de clasificación reales en el conjunto de prueba, apreciamos varias diferencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento del modelo se puede obtener con el método `score` evaluado sobre el conjunto de prueba `X_test` y comparado con los valores de clasificación reales `y_test`. A continuación calculamos el rendimiento de este modelo en el conjunto de entrenamiento, el conjunto de prueba y el conjunto de datos inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree3_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree3_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree3_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a ver varias formas de mejorar el rendimiento del modelo obtenido, reduciendo su sobreajuste. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prepoda: Máxima profundidad del árbol generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a considerar primero varias técnicas de prepoda que evitan el desarrollo del árbol de decisión de acuerdo con diferentes criterios.\n",
    "\n",
    "El primero que vamos a ver consiste en limitar la profundidad máxima del árbol generado. Esto se consigue con el parámetro `max_depth`, cuyo valor hay que establecer en el momento de construir la instancia del algoritmo de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree4_clf = DecisionTreeClassifier(random_state=10,max_depth=3)\n",
    "\n",
    "tree4_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al visualizar el árbol obtenido veremos que hemos conseguido detener su desarrollo a la profundidad indicada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree4_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principal consecuencia de esta prepoda es una disminución del rendimiento en el conjunto de entrenamiento, pero un aumento del rendimiento en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree4_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree4_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree4_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperar, el rendimiento en el conjunto de entrenamiento disminuye (si poda el rendimiento es `1`), aunque se obtiene un mejor rendimiento en el conjunto de prueba (ha subido de `0.74` a `0.81`). Sin embargo esto no siempre viene acompañado con una mejora en el rendimiento sobre el conjunto inicial (ha bajado sensiblemente) ni es lo que se pretende.\n",
    "\n",
    "Para buscar la profundidad donde se obtiene un mejor rendimiento en el conjunto de entrenamiento, habría que probar con distintos valores hasta encontrar el deseado. En el siguiente fragmento de código se muestra como se podría hacer en este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = 0\n",
    "best_score = 0\n",
    "for i in range(1,10):\n",
    "    tree_clf = DecisionTreeClassifier(random_state=10,max_depth=i)\n",
    "    tree_clf.fit(X_train,y_train)\n",
    "    score = tree_clf.score(X_test,y_test)\n",
    "    #print(\"Profundidad: \",i,\" - Score: \",score)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_depth = i\n",
    "print(\"Mejor Profundidad: \",best_depth,\" - Score: \",best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si construimos un árbol de decisión usando el parámetro de profundidad que hemos obtenido, podemos ver el aspecto que tiene y analizar su comportamiento en los conjuntos de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree4_clf = DecisionTreeClassifier(random_state=10,max_depth=2)\n",
    "\n",
    "tree4_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree4_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, conseguimos un modelo de clasificación con mejor rendimiento tanto en el conjunto de prueba como el conjunto de datos inicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree4_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree4_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree4_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, no se puede asegurar que una limitación en la profundidad del árbol de decisión construido vaya a mejorar siempre el rendimiento en el conjunto de prueba; ni que esa posible mejora tenga que venir acompañada de una mejora en el rendimiento en el conjunto inicial. Por eso, es conveniente considerar otros criterios para reducir el sobreajuste, hasta encontrar el más conveniente, si es que existe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepoda: Mínimo número de muestras en nodos internos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un segundo criterio que podemos considerar para podar el árbol de decisión durante su construcción consiste en limitar el número de muestras que debe tener un nodo para considerarlo un nodo interno del árbol y buscar un criterio de separación. Este valor se indica con el parámetro `min_samples_split` y se puede proporcionar como un valor absoluto (un número natural) o como una fracción del conjunto de muestras inicial (un número real entre 0 y 1). Si un nodo no tiene suficientes muestras asociadas entonces será considerado un nodo hoja cuyo valor de clasificación será el mayoritario entre sus muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree5_clf = DecisionTreeClassifier(random_state=10,min_samples_split=10)\n",
    "\n",
    "tree5_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree5_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se han dejado de desarrollar dos nodos en los que todavía no se había obtenido un valor de clasificación definitivo. Veamos como influye esto en el rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree5_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree5_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree5_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habría que probar con distintos valores del parámetro `min_samples_split` hasta encontrar el más adecuado, si es que existe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepoda: Mínimo grado de impureza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El último parámetro para el que vamos a ver un ejemplo de construcción del árbol de decisión es el mínimo grado de impureza exigido en un nodo para considerarlo un nodo interno y buscar un criterio de separación. Este criterio detiene el desarrollo del árbol cuando la proporción de la clase dominante en el conjunto de muestras asociado es muy alta en comparación con las de las otras clases. Este valor se indica con el parámetro `min_impurity_split` como un valor comprendido entre 0 y 1. Si la impureza de un nodo no alcanza el mínimo, entonces será considerado un nodo hoja cuyo valor de clasificación será el mayoritario entre sus muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree6_clf = DecisionTreeClassifier(random_state=10,min_impurity_split=0.1)\n",
    "\n",
    "tree6_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree6_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, se han dejado de desarrollar dos nodos en los que todavía no se había obtenido un valor de clasificación definitivo. Veamos como influye esto en el rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree5_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree5_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree5_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo, habría que probar con distintos valores del parámetro `min_impurity_split` hasta encontrar el más adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepoda: Otros criterios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro criterios que podemos usar para detener el desarrollo del árbol de decisión son:\n",
    "\n",
    "* `min_samples_leaf`: Un nodo será considerado como nodo interno si se puede encontrar un criterio de separación para dicho nodo que genere dos nodos con un número de muestras mayor o igual al valor indicado. El valor puede ser un número natural que indica una cantidad concreta o un número real entre 0 y 0.5 que indica una fracción del número de muestras inicial.\n",
    "* `max_features`: Indica el número máximo de características a considerar cuando se busca el mejor criterio de separación para un nodo interno.\n",
    "* `max_leaf_nodes`: Desarrolla el árbol de decisión hasta que se genera un número máximo de nodos hojas. Este desarrollo se realiza con una búsqueda por primero el mejor que valora los nodos con respecto a la reducción de la impureza.\n",
    "* `min_impurity_decrease`: Establece un umbral para la reducción de la impureza en un nodo interno. Un nodo será considerado como nodo interno si se puede encontrar un criterio de separación que produce una reducción de la impureza mayor o igual que la indicada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postpoda: Modificar el árbol de decisión generado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de reducir el sobreajuste consiste en realizar una poda a posteriori del árbol de decisión generado. Esta forma de poda no está incluida en la librería *scikit_learn*, pero podemos aplicarla si entendemos la estructura de datos que se utiliza para almacenar el árbol de decisión.\n",
    "\n",
    "Consideremos el árbol de decisión obtenido con el conjunto de entrenamiento y sin ningún tipo de poda durante el proceso de construcción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree3_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El árbol de decisión está almacenado en el parámetro [`tree_`](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html) del objeto instancia del algoritmo de aprendizaje.\n",
    "\n",
    "En este parámetro podemos encontrar el número de nodos del árbol de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3_clf.tree_.node_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También contiene varios arrays que almacenan la estructura del árbol de decisión: cada nodo tiene asociado un índice comenzando en `0` para el nodo raíz; el array `children_left` contiene los índices de los hijos izquierdos de los nodos internos (los nodos hoja tienen el valor `-1`); el array `children_right` contiene los índices de los hijos derechos de los nodos internos (de nuevo, los nodos hoja tienen el valor `-1`); y el array `feature` contiene el índice de la característica considerada en el criterio de separación para los nodos internos (los nodos hoja tienen el valor `-2`).\n",
    "\n",
    "Por ejemplo, en el árbol de decisión anterior estos valores son:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3_clf.tree_.children_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3_clf.tree_.children_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3_clf.tree_.feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El nodo con índice `0` es el nodo raíz del árbol, su hijo izquierdo es el que tiene índice `1` (`children_left[0] = 1`), su hijo derecho es el que tiene índice `2` (`children_right[0] = 2`) y la característica que se considerada en el criterio de separación asociado es la de índice `3` (`feature[0] = 3`).\n",
    "\n",
    "El nodo con índice `1` es el hijo izquierdo del nodo raíz, se trata de un nodo hoja (`children_left[1] = -1`, `children_right[1] = -1` y `feature[1] = -2`).\n",
    "\n",
    "El nodo con índice `2` es el hijo derecho del nodo raíz, su hijo izquierdo es el que tiene índice `3`, su hijo derecho es el que tiene índice `6` y la característica considerada en el criterio de separación asociado es la de índice `2`.\n",
    "\n",
    "Para podar a posteriori un árbol de decisión en un nodo concreto, basta con asignar el valor `-1` a la entrada correspondiente en los arrays `children_left` y `children_right` y el valor `-2` a la del array `feature`. Vamos a verlo en este caso podando el árbol en los nodos de índices `3` y `6`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar la poda en el árbol de decisión original sin perderlo, para poder hacer otras podas y comparar resultados, vamos a hacer una copia de toda su estructura con la función `deepcopy` de la librería `copy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "tree3A_clf = deepcopy(tree3_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora cambiamos los valores asociados al índice del nodo que queremos podar (el `3`) en los arrays `children_left`, `children_right` y `feature`, asignándole los valores `-1`, `-1` y `-2`, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3A_clf.tree_.children_left[3] = -1\n",
    "tree3A_clf.tree_.children_right[3] = -1\n",
    "tree3A_clf.tree_.feature[3] = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si visualizamos el resultado podemos comprobar el efecto de la poda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree3A_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como ha afectado la poda al rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree3A_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree3A_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree3A_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a repetir otra vez el experimento, ahora para podar el nodo con índice `6`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3B_clf = deepcopy(tree3_clf)\n",
    "\n",
    "tree3B_clf.tree_.children_left[6] = -1\n",
    "tree3B_clf.tree_.children_right[6] = -1\n",
    "tree3B_clf.tree_.feature[6] = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si visualizamos el resultado podemos comprobar el efecto de la poda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree3B_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos como ha afectado esta segunda poda al rendimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree3B_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree3B_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree3B_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar el experimento, vamos a podar ambos nodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree3AB_clf = deepcopy(tree3_clf)\n",
    "\n",
    "tree3AB_clf.tree_.children_left[3] = -1\n",
    "tree3AB_clf.tree_.children_right[3] = -1\n",
    "tree3AB_clf.tree_.feature[3] = -2\n",
    "tree3AB_clf.tree_.children_left[6] = -1\n",
    "tree3AB_clf.tree_.children_right[6] = -1\n",
    "tree3AB_clf.tree_.feature[6] = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizamos el resultado para comprobar el efecto de la poda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(tree3AB_clf,iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El efecto de ambas podas en el rendimiento es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",tree3AB_clf.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",tree3AB_clf.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",tree3AB_clf.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Random Forest*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora otra técnica para reducir el sobreajuste, la combinación de modelos. En la biblioteca *scikit_learn* disponemos de una implementación de la técnica *Random Forest* con la clase `RandomForestClassifier` de la librería `ensemble`. En este caso, además de los criterios para limitar el desarrollo del árbol de decisión que hemos visto antes, se incluye el parámetro `n_estimators` con el que se índica el número de árboles que se van a construir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos este algoritmo de aprendizaje con el conjunto de datos de entrenamiento usando el método `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método `predict` devuelve la clase predicha para un conjunto de muestras. En este caso lo usamos para las 10 primeras muestras del conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.predict(X_test[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenada, la clase `RandomForestClassifier` almacena en el parámetro `estimators_` un array con los árboles de decisión que forman el conjunto. Estos objetos se pueden utilizar de forma independiente, por ejemplo para ver la clase predicha para un conjunto de muestras. Si vemos los resultados para las 10 primeras muestras del conjunto de prueba podemos confirmar que la predicción del conjunto de árboles es la de la clase mayoritaria entre las predicciones independientes de los árboles de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    print(forest.estimators_[i].predict(X_test[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El efecto en el rendimiento es bastante mejor que en casos anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",forest.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",forest.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",forest.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar cualquiera de los árboles que forman el conjunto a partir de la entrada correspondiente del parámetro `estimators_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(forest.estimators_[3],iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gradient Boosting*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La segunda técnica de combinación de modelos que hemos visto es *Gradient Boosted Regresion Trees*. En la biblioteca scikit_learn disponemos de una implementación de esta técnica con la clase `GradientBoostingClassifier` de la librería ensemble. De nuevo, con el parámetro `n_estimators` indicamos el número de árboles que se van a construir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "ensemble = GradientBoostingClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos este algoritmo de aprendizaje con el conjunto de datos de entrenamiento usando el método `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El rendimiento de este clasificador en los conjuntos de datos es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rendimiento en el conjunto de entrenamiento: \",ensemble.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",ensemble.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",ensemble.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como el rendimiento en el conjunto de entrenamiento es del 100%, es muy probable que el modelo esté sobreajustado. Podemos reducir el sobreajuste con cualquier parámetro de los vistos para realizar poda en los árboles generados (por ejemplo `max_depth`) o con parámetros específicos del proceso de construcción del conjunto de clasificadores (por ejemplo `learning_rate`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = GradientBoostingClassifier(n_estimators=10,max_depth=1)\n",
    "\n",
    "ensemble.fit(X_train,y_train)\n",
    "\n",
    "print(\"Rendimiento en el conjunto de entrenamiento: \",ensemble.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",ensemble.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",ensemble.score(X_data,y_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = GradientBoostingClassifier(n_estimators=10,learning_rate=0.02)\n",
    "\n",
    "ensemble.fit(X_train,y_train)\n",
    "\n",
    "print(\"Rendimiento en el conjunto de entrenamiento: \",ensemble.score(X_train,y_train))\n",
    "print(\"Rendimiento en el conjunto de prueba: \",ensemble.score(X_test,y_test))\n",
    "print(\"Rendimiento en el conjunto total: \",ensemble.score(X_data,y_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
