{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado e ingeniería de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesión de trabajo con Jupyter aplicaremos los conceptos presentados en el módulo sobre preprocesado e ingeniería de características a los conjuntos de datos Titanic e Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulación de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes ejercicios vamos a utilizar el conjunto de datos [Titanic](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.html) para realizar distintos tipos de modificaciones en los datos: rellenado de valores ausentes y modificación de características categóricas mediante correspondencia numérica, codificación *one-hot* y escalado. El conjunto de datos Titanic contiene información sobre 1309 pasajeros del Titanic el día del naufragio, indicando 14 características como su nombre, sexo, edad, número de ticket, tarifa, puerto de embarque, clase en la que viajaba y si sobrevivió al desastre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos está en formato CSV así que utlizaremos la librería Pandas para leerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque podemos coger el conjunto de datos de la página web de la Universidad Vanderbilt, vamos a usar una versión ligeramente modificada por A. Müller para su libro \"Introducción to Machine Learning with Python\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_url = ('https://raw.githubusercontent.com/amueller/'\n",
    "               'scipy-2017-sklearn/091d371/notebooks/datasets/titanic3.csv')\n",
    "df = pd.read_csv(titanic_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener una idea de cómo es este conjunto de datos podemos visualizar los 5 primeros registros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio utilizaremos las características clase (pclass), sexo (sex), edad (age), tarifa (fare) y puerto de embarque (embarked). Clase es una característica numérica que puede tomar tres valores, 1, 2 o 3, correspondiendo con la clase en la que se encontraba el pasajero. Edad y tarifa son características numéricas continuas. Sexo y puerto de embarque son características de tipo categórico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['pclass','sex','age','fare','embarked']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores ausentes en los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero vamos a analizar los datos para comprobar si existen valores ausentes y cuántos hay. Para esto usamos el método \n",
    "`isnull()` de los *dataframe* de Pandas, que crea un índice de los valores nulos; y a continuación contamos cuantos valores nulos hay para cada característica con el método `sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver hay una gran cantidad de valores ausentes para la característica edad y uno o dos para las características tarifa y puerto de embarque."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La primera forma de tratar con valores ausentes consiste en eliminarlos. Esto es fácil de hacer usando la librería Pandas. Simplemente hay que usar el método `dropna()` de los *dataframe* para conseguirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a ver medidas menos drásticas para tratar con los valores ausentes, pero antes vamos a crear un índice de los valores ausentes de la característica edad, para ver el efecto de los cambios que vamos a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = data['age'].isnull()\n",
    "\n",
    "data[missing][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a rellenar los valores ausentes de la característica edad con la media de todos los datos disponibles. El valor de la media se puede obtener con el método `mean()` de los *dataframe*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rellenar los campos vacíos de una característica vamos a utilizar la clase `SimpleImputer` de la librería `impute` de *Scikit-learn*. Esta clase nos permite rellenar los campos vacíos de un conjunto de datos con distintas estrategias, como la media, la mediana o la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El inconveniente es que la clase `SimpleImputer` actúa sobre todo el conjunto de datos, usando la misma estrategia para rellenar todos los huecos vacíos de todas las características. Si usamos esta clase con el objetivo de rellenar los campos vacíos de la característica edad con la media, se producirá un error al tratar de rellenar los campos vacíos de características categóricas. La única estrategia válida para cualquier característica, independientemente de si es categórica o numérica, es la moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si = SimpleImputer(strategy='mean')\n",
    "\n",
    "#si.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La versión 0.20.2 de *Scikit-learn* incorpora la clase `ColumnTransformer` con el objetivo de hacer transformaciones en un conjunto de datos especificando las columnas sobre las que queremos actuar. Se encuentra en la librería `compose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el constructor de la clase `ColumnTransformer` hay que proporcionar una lista de ternas formadas por un nombre único asociado a la transformación para un conjunto de columnas, la transformación que queremos realizar (en este caso rellenar los campos vacíos con la media) y las columnas sobre las que queremos actuar (en este caso la característica edad). Podemos indicar varias columnas y la transformación se llevará a cabo sobre todas ellas de forma independiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"media\",SimpleImputer(strategy='mean'),['age'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos esta instancia de la clase `ColumnTransformer` con los datos y podemos ver el resultado sobre los ejemplos que contenían valores vacíos para la característica edad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ct.fit_transform(data)\n",
    "\n",
    "X[missing][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, se ha producido la transformación, pero hemos perdido el resto de columnas de nuestro conjunto de datos. Esto es porque la clase `ColumnTransformer` actúa sobre las características que se indican, eliminando todas las demás. Si queremos modificar este comportamiento, podemos hacerlo usando el parámetro `remainder`, que sirve para especificar qué hacer con el resto de las características, con el valor `passthrough`, que indica que se deben dejar sin modificar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"media\",SimpleImputer(strategy='mean'),['age'])],\n",
    "                        remainder='passthrough')\n",
    "\n",
    "X = ct.fit_transform(data)\n",
    "\n",
    "X[missing][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra cosa que observamos ahora es que se ha alterado el orden de las características. Esto se debe a que la clase `ColumnTransformer` deja las características en el orden en que se indican en la lista de transformaciones. Si queremos mantener el orden original, deberíamos indicar una transformación nula para el resto de características distintas de la edad, en el orden que queramos mantener. La transformación nula se indica usando el valor `'passthrough'` como segundo campo de las ternas que indican cómo actuar sobre las columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass','sex']),\n",
    "                        (\"media\",SimpleImputer(strategy='mean'),['age']),\n",
    "                        (\"original2\",'passthrough',['fare','embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ct.transform(data)[missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a rellenar valores ausentes usando otro criterio, en concreto la mediana para la característica tarifa. La mediana de un conjunto de datos se puede obtener con el método `median()` de los *dataframe*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['fare'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer esta modificación es tan simple como indicarla en una instancia de la clase `ColumnTransformer` sobre el dato correspondiente. En este caso ampliamos la instancia anterior para hacer esta segunda modificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass','sex']),\n",
    "                         (\"si1\",SimpleImputer(strategy='mean'),['age']),\n",
    "                         (\"si2\",SimpleImputer(strategy='median'),['fare']),\n",
    "                         (\"original2\",'passthrough',['embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vamos a rellenar los valores ausentes de la característica puerto de embarque con la moda. La moda de un conjunto de datos se puede obtener con el método `mode()` de los *dataframe*. En el resultado se muestran todos los valores que tiene un máximo de frecuencia de aparición en el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embarked'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente incluimos esta transformación para la característica puerto de embarque en la instancia de la clase `ColumnTransformer`. Una vez hecho esto, entrenamos la clase con nuestro conjunto de datos y creamos una versión modificada para usarla en el resto de esta hoja de trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass','sex']),\n",
    "                        (\"si1\",SimpleImputer(strategy='mean'),['age']),\n",
    "                        (\"si2\",SimpleImputer(strategy='median'),['fare']),\n",
    "                        (\"si3\",SimpleImputer(strategy='most_frequent'),['embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Características categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a tratar ahora con transformaciones de características categóricas: correspondencia numérica y codificación *one-hot*. Aunque más adelante usaremos de nuevo la clase `ColumnTransformer` para integrar otras transformaciones de los datos, esto no es posible hacerlo para la correspondencia numérica, que tendremos que hacer sobre cada columna de forma aislada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el conjunto de valores que toma una característica con el método `unique()` de los *dataframe*. En este caso podemos comprobar que la característica sexo toma unicamente dos valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En *Scikit-learn* podemos hacer una correspondencia numérica con la clase `LabelEncoder` de la librería `preprocessing`. Al entrenar una instancia de esta clase con un conjunto de datos, se identifican los valores o clases que existen y, posteriormente, se pueden reemplazar por valores numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de clases identificadas se almacena en el campo `classes_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos transformar los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,1] = le.transform(X[:,1])\n",
    "\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La codificación *one-hot* se consigue utilizando la clase `OneHotEncoder` de la librería `preprocessing`. En este caso podemos integrar esta transformación como un modificador de una instancia de la clase `ColumnTransformer` sobre las columnas que nos interesen. Vamos a hacerlo primero sobre la característica sexo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass']),\n",
    "                        (\"ohe\",OneHotEncoder(),['sex']),\n",
    "                        (\"si1\",SimpleImputer(strategy='mean'),['age']),\n",
    "                        (\"si2\",SimpleImputer(strategy='median'),['fare']),\n",
    "                        (\"si3\",SimpleImputer(strategy='most_frequent'),['embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer lo mismo sobre la característica puerto de embarque no es tan simple, puesto que ya habíamos indicado una modificación para esta característica. Para conseguir hacer una segunda modificación sobre una característica vamos a usar la clase `Pipeline` de la librería `pipeline`. Con esta clase podemos indicar una secuencia de transformaciones a realizar sobre un conjunto de datos. Estas transformaciones se realizan de forma secuencial sobre los datos e incluso se podría indicar como paso final un modelo de decisión. En nuestro caso, creamos una instancia de la clase `Pipeline` para rellenar primero los valores ausentes con el valor más frecuente (la moda) y después hacemos una codificación *one-hot*.\n",
    "\n",
    "A continuación, usamos la instancia de la clase `Pipeline` que acabamos de crear como transformador asociado a la característica puerto de embarque en una instancia de la clase `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "impohe = Pipeline([('si3', SimpleImputer(strategy='most_frequent')),\n",
    "                   ('onehot', OneHotEncoder())])\n",
    "\n",
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass']),\n",
    "                        (\"ohe\",OneHotEncoder(),['sex']),\n",
    "                        (\"si1\",SimpleImputer(strategy='mean'),['age']),\n",
    "                        (\"si2\",SimpleImputer(strategy='median'),['fare']),\n",
    "                        (\"impohe\",impohe,['embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Escalado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El último tipo de transformación que vamos a ver en este ejemplo es el escalado, que puede ser de dos tipos, normalización o estandarización. \n",
    "\n",
    "La normalización se consigue con la clase `MinMaxScaler` de la librería `preprocessing`. En nuestro ejemplo, usamos normalización para escalar los datos de la edad. Para conseguir hacer esto después de haber rellenado los valores ausentes de esta característica con la media, usamos una nueva instancia de la clase `Pipeline` indicando las dos transformaciones que queremos realizar y usamos esta instancia como transformador para la característica edad en una instancia de la clase `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pip1 = Pipeline([('si1', SimpleImputer(strategy='mean')),\n",
    "                 ('scaler1',MinMaxScaler())])\n",
    "\n",
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass']),\n",
    "                        (\"ohe1\",OneHotEncoder(),['sex']),\n",
    "                        (\"pip1\",pip1,['age']),\n",
    "                        (\"si2\",SimpleImputer(strategy='median'),['fare']),\n",
    "                        (\"impohe\",impohe,['embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estandarización se consigue con la clase `StandardScaler` de la librería `preprocessing`. En nuestro ejemplo, usamos estandarización para escalar los datos de la tarifa. Para conseguir hacer esto después de haber rellenado los valores ausentes de esta característica con la mediana, usamos una nueva instancia de la clase `Pipeline` indicando las dos transformaciones que queremos realizar y usamos esta instancia como transformador para la característica tarifa en una instancia de la clase `ColumnTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pip2 = Pipeline([('si2', SimpleImputer(strategy='median')),\n",
    "                 ('scaler2',StandardScaler())])\n",
    "\n",
    "ct = ColumnTransformer([(\"original1\",'passthrough',['pclass']),\n",
    "                        (\"ohe1\",OneHotEncoder(),['sex']),\n",
    "                        (\"pip1\",pip1,['age']),\n",
    "                        (\"pip2\",pip2,['fare']),\n",
    "                        (\"impohe\",impohe,['embarked'])])\n",
    "\n",
    "X = ct.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver el resultado final comparándolo los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Datos originales: \")\n",
    "print(data[0:5])\n",
    "\n",
    "print(\"Datos transformados: \")\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de la dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes ejercicios vamos a utilizar de nuevo el conjunto de datos [Iris](https://es.wikipedia.org/wiki/Iris_flor_conjunto_de_datos) que contiene información de tres especies de la flor Iris: Iris setosa, Iris virginica e Iris versicolor. Recordemos que en este conjunto de datos hay información de 150 muestras de flores, 50 de cada tipo, sobre el largo y el ancho del pétalo y el sépalo, medidos en centímetros. El objetivo es aplicar distintas técnicas de reducción de dimensionalidad a este conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el conjunto de datos Iris incluido en Scikit-learn, usando la función [`load_iris()`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) de la biblioteca `sklearn.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer es escalar todos los datos usando la clase `StandardScaler` para conseguir un estandarizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(iris.data)\n",
    "iris.data = scaler.transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiamos el array de nombres de las características, ya que ahora no son valores medidos en centímetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names = ['longitud de sépalo','anchura de sépalo',\n",
    "                      'longitud de pétalo','anchura de pétalo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almacenamos los campos que resultan de interés para este ejercicio en variables distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, y_data, X_names, y_names = \\\n",
    "    iris.data, iris.target, iris.feature_names, iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a recordar cómo se distribuían estos datos y para ello los visualizaremos usando la librería `matplotlib`. Para esto, consideramos un par de características y presentamos en un gráfico 2D un conjunto de puntos que representan cada uno de los ejemplos del conjunto de datos. Para diferenciar los valores de clasificación usamos distintas formas y colores para cada uno de ellos: cuadrados rojos para el primer valor de clasificación (Iris setosa), círculos verdes para el segundo valor de clasificación (Iris versicolor) y rombos azules para el tercer valor de clasificación (Iris virgínica)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos la biblioteca [`pyplot`](https://matplotlib.org/api/pyplot_api.html) de la librería `matplotlib` con el nombre `plt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso para dibujar los datos consiste en limpiar el lienzo (`plt.clf()`) y para cada terna formada por el valor de clasificación (`range(len(clases))`), la forma (`\"soD\"`) y el color (`\"rgb\"`) deseados, recoger los valores de las características a representar (`c1` y `c2`) de todos los datos con el valor de clasificación considerado. Estos puntos se representan con el método (`plt.scatter`) con la forma y el color fijados. Finalmente etiquetamos los ejes horizontal y vertical con los nombres de las características representadas.\n",
    "\n",
    "Este proceso se puede definir como el efecto de una función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion_grafica(datos,caracteristicas,objetivo,clases,c1,c2):\n",
    "    for tipo,marca,color in zip(range(len(clases)),\"soD\",\"rgb\"):\n",
    "        plt.scatter(datos[objetivo == tipo,c1],\n",
    "                    datos[objetivo == tipo,c2],marker=marca,c=color)\n",
    "    plt.xlabel(caracteristicas[c1])\n",
    "    plt.ylabel(caracteristicas[c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora utilizamos esta función para representar gráficamente la distribución de los datos con respecto a cada pareja de características. Eso se consigue dividiendo el lienzo en seis trozos, en 2 filas y 3 columnas con las correspondientes llamadas al método `plt.subplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion_conjunta(X_data,X_names,y_data,y_names):\n",
    "    plt.clf()\n",
    "    plt.rcParams[\"figure.figsize\"] = [18,12]\n",
    "    plt.subplot(231)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,0,1)\n",
    "    plt.subplot(232)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,0,2)\n",
    "    plt.subplot(233)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,0,3)\n",
    "    plt.subplot(234)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,1,2)\n",
    "    plt.subplot(235)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,1,3)\n",
    "    plt.subplot(236)\n",
    "    representacion_grafica(X_data,X_names,y_data,y_names,2,3)\n",
    "    plt.subplots_adjust(wspace=0.2,hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "representacion_conjunta(X_data,X_names,y_data,y_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vamos a ver cómo se comporta un modelo de decisión cualquiera para este conjunto de datos, para tener una referencia de su rendimiento. En este caso usamos un modelo de clasificación basado en vectores soporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "modelo0 = LinearSVC().fit(X_data,y_data)\n",
    "\n",
    "modelo0.score(X_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación recursiva de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La eliminación recursiva de características es una técnica de selección de características que consiste en construir modelos de decisión con el conjunto de datos original al que se ha quitado una característica, para cada una de las características disponibles. De estos modelos se escoge el que tenga mejor rendimiento, y se continúa de la misma forma con las características restantes. El proceso termina cuando se ha llegado a un número de características objetivo o mientras que se obtengan modelos que mejoren el rendimiento.\n",
    "\n",
    "Vamos a hacerlo con el conjunto de datos Iris. Para esto vamos a considerar cuatro situaciones distintas, cada una de ellas como resultado de eliminar una característica del conjunto de datos original. Entrenamos un modelo de clasificación basado en vectores soporte para cada una de los conjuntos de datos resultantes y evaluamos su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1A_data = [[x[1],x[2],x[3]] for x in X_data]\n",
    "\n",
    "modelo1A = LinearSVC().fit(X1A_data, y_data)\n",
    "\n",
    "modelo1A.score(X1A_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1B_data = [[x[0],x[2],x[3]] for x in X_data]\n",
    "\n",
    "modelo1B = LinearSVC().fit(X1B_data, y_data)\n",
    "\n",
    "modelo1B.score(X1B_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1C_data = [[x[0],x[1],x[3]] for x in X_data]\n",
    "\n",
    "modelo1C = LinearSVC().fit(X1C_data, y_data)\n",
    "\n",
    "modelo1C.score(X1C_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1D_data = [[x[0],x[1],x[2]] for x in X_data]\n",
    "\n",
    "modelo1D = LinearSVC().fit(X1D_data, y_data)\n",
    "\n",
    "modelo1D.score(X1D_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el mejor resultado se obtiene eliminando la segunda característica. Repetimos el proceso para el conjunto de datos en el que se ha eliminado esta característica, buscando una segunda simplificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1A1_data = [[x[2],x[3]] for x in X_data]\n",
    "\n",
    "modelo1A1 = LinearSVC().fit(X1A1_data, y_data)\n",
    "\n",
    "modelo1A1.score(X1A1_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1A2_data = [[x[0],x[3]] for x in X_data]\n",
    "\n",
    "modelo1A2 = LinearSVC().fit(X1A2_data, y_data)\n",
    "\n",
    "modelo1A2.score(X1A2_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1A3_data = [[x[0],x[2]] for x in X_data]\n",
    "\n",
    "modelo1A3 = LinearSVC().fit(X1A3_data, y_data)\n",
    "\n",
    "modelo1A3.score(X1A3_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez eliminada la segunda característica, la mejor opción consiste en eliminar la primera. Si el objetivo fuese reducir el conjunto de características mientras haya mejoras en el rendimiento, entonces nos quedaríamos con las características primera, tercera y cuarta. Si el objetivo fuese reducir el conjunto de características a la mitad, entonces nos quedaríamos con las características tercera y cuarta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scikit-learn* implementa una reducción recursiva de las características, eliminando en cada paso la peor caracterísica, según el grado que establezca un modelo de decisión. Esto se consigue con la clase `RFE` de la librería `feature_selection`, indicando el modelo a usar para escoger la mejor característica en cada paso como argumento en el constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "modelo1 = LinearSVC()\n",
    "\n",
    "selector = RFE(modelo1,n_features_to_select=1,step=1)\n",
    "\n",
    "selector.fit(X_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el campo `support_` se almacena un índice de las características que se obtienen como mejores. Al igual que lo ocurrido con nuestro ejemplo, se llega a la conclusión de que las mejores características son las relacionadas con el pétalo de la flor (las dos últimas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de características basada en modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scikit-learn* también implementa la selección de características basada en modelos. Se trata de la clase `SelectFromModel` de la librería `feature_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos seleccionar las mejores características a partir de un modelo Random Forest con una cantidad adecuada de estimadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "select1 = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "select1.fit(X_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método `get_support()` podemos obtener un índice de las mejores características encontradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select1.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos seleccionar las mejores características a partir de un modelo lineal con regularización L1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "select2 = SelectFromModel(LinearSVC(C=0.01, penalty=\"l1\", dual=False))\n",
    "\n",
    "select2.fit(X_data,y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el modelo detecta que las mejores características son las tres últimas, pero el resultado cambia con valores diferentes del parámetro `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select2.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de componentes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar, vamos a ver como realizar un análisis de componentes principales en *Scikit-learn*. Esto se consigue con la clase `PCA` de la librería `decomposition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El número de componentes que queremos dejar en el resultado se puede indicar como valor del parámetro `n_components`. En este caso hemos dejado el valor 4 para ver la importancia relativa que tiene cada una de las componentes obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "\n",
    "pca.fit(X_data)\n",
    "\n",
    "X_pca = pca.transform(X_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los autovalores asociados a las componentes principales, es decir, el grado de importancia de estas componentes, se almacenan en el campo `explained_variance_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar la última componente tiene muy poca importancia comparada con las otras.\n",
    "\n",
    "Las componentes principales, es decir, los autovectores de la matriz de covarianzas, se almacenan en el campo `components_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si hacemos una representación conjunta de las cuatro componentes podemos ver el grado de correlacion entre ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representacion_conjunta(X_pca,['componente 1','componente 2','componente 3','componente 4'],\n",
    "                        y_data,y_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que parece que la última componente es prescindible, podemos quedarnos con las tres primeras y modificar el conjunto de datos original para entrenar un modelo de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "X_pca = pca.fit_transform(X_data)\n",
    "\n",
    "modeloPCA = LinearSVC().fit(X_pca, y_data)\n",
    "\n",
    "modeloPCA.score(X_pca,y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
